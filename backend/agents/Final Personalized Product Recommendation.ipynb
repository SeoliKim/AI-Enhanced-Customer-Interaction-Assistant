{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee9db0f-5863-40f1-9104-1c33c0da10ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Found existing installation: numpy 1.26.0\n",
      "Uninstalling numpy-1.26.0:\n",
      "  Successfully uninstalled numpy-1.26.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/Users/jocelyn/miniconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (53 kB)\n",
      "Using cached numpy-1.26.0-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/Users/jocelyn/miniconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tableone 0.9.1 requires openpyxl>=3.1.2, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --yes\n",
      "Requirement already satisfied: gensim in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "!pip install scikit-learn \n",
    "!pip uninstall numpy --yes \n",
    "!pip install numpy==1.26.0 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score \n",
    "!pip install gensim --yes \n",
    "!pip install gensim \n",
    "import gensim \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import time \n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea949d9-5b3f-4e37-b53a-805b8e66ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/jocelyn/Desktop/DXC-4 Technology /AI-Enhanced-Customer-Interaction-Assistant/data/Final_db.csv\" \n",
    "df = pd.read_csv(filename, header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699dbaaa-2f48-45fe-a8b8-386e51bc4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are products with multiple orders \n",
    "product_id_value_counts = df[\"product_id\"].value_counts() \n",
    "\n",
    "# store the products with multiple orders in products_multiple_orders \n",
    "products_multiple_orders = product_id_value_counts[product_id_value_counts > 1] \n",
    "\n",
    "# create a new column in df that stores the number of times each item was purchased \n",
    "df[\"times_purchased\"] = df[\"product_id\"].map(product_id_value_counts) \n",
    "\n",
    "# stores all of the unique product categories into the array \"product_categories\" \n",
    "product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "\n",
    "# fill in rows in df[\"review_comment_title\"] column that have na values with \"\" \n",
    "df[\"review_comment_title\"] = df[\"review_comment_title\"].fillna(\"\") \n",
    "\n",
    "# fill in rows in df[\"review_comment_message\"] column that have na values with \"\" \n",
    "df[\"review_comment_message\"] = df[\"review_comment_message\"].fillna(\"\") \n",
    "\n",
    "# merge the product reviews and product review titles with a space in between both \n",
    "df[\"merged_review_messages_and_titles\"] = df[\"review_comment_message\"] + \" \" + df[\"review_comment_title\"] \n",
    "df[\"merged_review_messages_and_titles\"] = df[\"merged_review_messages_and_titles\"].str.strip() \n",
    "\n",
    "# dictionary to store dataframes of the product reviews for all products in each category \n",
    "# products are separated by category \n",
    "product_reviews_by_category = {}; \n",
    "\n",
    "# fill in dictionary with keys (df category) and values (product_id and product reviews for all products in that category) \n",
    "for category in product_categories: \n",
    "    product_reviews_by_category[\"df_\" + str(category)] = df[df[\"product_cat_name_translated\"] == category][[\"product_id\", \"merged_review_messages_and_titles\"]] \n",
    "\n",
    "# feature ~ product title and review \n",
    "X = df[\"merged_review_messages_and_titles\"] \n",
    "\n",
    "# label ~ product category \n",
    "y = df[\"product_cat_name_translated\"] \n",
    "\n",
    "# preprocess each (each row) product title and review \n",
    "X = X.apply(lambda row: gensim.utils.simple_preprocess(row)) \n",
    "\n",
    "# split data into a training set and a test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1111) \n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(X_train, vector_size=100, window=5, min_count=1) \n",
    "\n",
    "# replace every word in the dataset that's also found in word2vec_model with their word embedding \n",
    "# a set with all of the unique words in the word2vex_model \n",
    "words = set(word2vec_model.wv.index_to_key) \n",
    "\n",
    "# X_train word embeddings \n",
    "# array for all of the word embeddings for X_train \n",
    "X_train_word_embeddings = []; \n",
    "for row in X_train: \n",
    "    row_embedding = [] \n",
    "    for word in words: \n",
    "        if word in row: \n",
    "            row_embedding.append(word2vec_model.wv[word]) \n",
    "    X_train_word_embeddings.append(np.array(row_embedding)) \n",
    "\n",
    "# X_test word embeddings \n",
    "# array for all of the word embeddings for X_test \n",
    "X_test_word_embeddings = []; \n",
    "for row in X_test: \n",
    "    row_embedding = []; \n",
    "    for word in words: \n",
    "        if word in row: \n",
    "            row_embedding.append(word2vec_model.wv[word]) \n",
    "    X_test_word_embeddings.append(np.array(row_embedding)) \n",
    "\n",
    "X_train_feature_vector = []\n",
    "for w in X_train_word_embeddings:\n",
    "    if w.size:\n",
    "        X_train_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_train_feature_vector.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_feature_vector = []\n",
    "for w in X_test_word_embeddings:\n",
    "    if w.size:\n",
    "        X_test_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_test_feature_vector.append(np.zeros(100, dtype=float))\n",
    "\n",
    "# logistic regression \n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_feature_vector, y_train)\n",
    "\n",
    "probability_predictions = model.predict_proba(X_test_feature_vector) \n",
    "\n",
    "# predict most likely product category \n",
    "class_label_predictions = model.predict(X_test_feature_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b2e8f85-40e2-432d-9503-e3b24260b3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How about this product?\n",
      "Product:  5ac9d9e379c606e36a8094a6046f75dc\n",
      "Price:  109.9\n",
      "Average Product Rating:  1.3333333333333333\n",
      "here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Product Rating: \u001b[39m\u001b[38;5;124m\"\u001b[39m, products_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m---> 87\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDo you want more detail about this product? (Y/N)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     89\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m more_detail_user_input\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mstrip() \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "    else: \n",
    "        while len(token_vectors) == 0: \n",
    "            user_input = input(\"\\nWe can't find a related product. Please input more detail.\") \n",
    "            user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "            token_vectors = [] \n",
    "    \n",
    "            # loop through all tokens from the tokenized version of the user input \n",
    "            for token in user_input_tokens: \n",
    "                # if the token is also found in the dataset \n",
    "                if token in word2vec_model.wv.key_to_index: \n",
    "                    # add its word vector to the \"token_vectors\" list \n",
    "                    token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "            # if there are vectors \n",
    "            if len(token_vectors) > 0: \n",
    "                # calculate the mean for each token_vector and combine into an array \n",
    "                overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "                # reshape into a two dimensional array \n",
    "                overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "                word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "    word2vec_prediction_str = \"df_\" + str(word2vec_prediction[0]) \n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    # use bigrams (for cases of negation like \"not good\") and trigrams (for cases with numbers like \"bought 3 pairs\") \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    # fit vectorizer to the product review titles and messages \n",
    "    vectorizer.fit(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "    \n",
    "    # result_matrix contains each document's TF_IDF scores \n",
    "    resulting_matrix = vectorizer.transform(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "        \n",
    "    product_id = product_reviews_by_category[word2vec_prediction_str][\"product_id\"].tolist() \n",
    "        \n",
    "    resulting_matrix = pd.DataFrame(resulting_matrix.toarray(), index=product_id, columns=vectorizer.get_feature_names_out()) \n",
    "    \n",
    "    # determine product in category that's most similar to user input \n",
    "    user_input_TF_IDF_vector = vectorizer.transform([user_input]) \n",
    "    # similarity of user input word vector and each of the rows' word vectors \n",
    "    similarity = cosine_similarity(user_input_TF_IDF_vector, resulting_matrix) \n",
    "    greatest_similarity_indices = np.argsort(similarity[0][::-1]) \n",
    "    greatest_similarity_product_ids = resulting_matrix.index[greatest_similarity_indices] \n",
    "    greatest_similarity_product_ids_list = greatest_similarity_product_ids.unique().tolist() \n",
    "    \n",
    "    # rows with that product id \n",
    "    products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[0]] \n",
    "    \n",
    "    clear_output(wait=True) \n",
    "    \n",
    "    print(\"\\nHow about this product?\") \n",
    "    print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "    print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "    print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "    more_detail_user_input = input(\"\\nDo you want more detail about this product? (Y/N)\") \n",
    "    more_detail_user_input = more_detail_user_input.upper().strip() \n",
    "    \n",
    "    more_recommendations = \"N\" \n",
    "    more_detail = \"\" \n",
    "    moree = False \n",
    "    user_input_product_cat = \"\" \n",
    "    user_input = \"\" \n",
    "    \n",
    "    if more_detail_user_input == \"Y\": \n",
    "        print(\"\\nOk. Here is more detail about the product.\") \n",
    "        print(\"Total Orders: \", len(products_df)) \n",
    "        print(\"Product Weight (g): \", products_df[\"product_weight_g\"].iloc[0]) \n",
    "        print(\"Product Length (cm): \", products_df[\"product_length_cm\"].iloc[0]) \n",
    "        print(\"Product Height (cm): \", products_df[\"product_height_cm\"].iloc[0]) \n",
    "        print(\"Product Width (cm): \", products_df[\"product_width_cm\"].iloc[0]) \n",
    "        \n",
    "        good = input(\"\\nIs this a good recommendation? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "            \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "                break \n",
    "        \n",
    "        else: \n",
    "            print(\"\\nWe're sorry.\") \n",
    "            token_vectors = []; \n",
    "    \n",
    "    else: \n",
    "        more_recommendations = input(\"\\nWould you like more similar product recommendations? (Y/N)\") \n",
    "        more_recommendations = more_recommendations.upper().strip() \n",
    "    \n",
    "    if more_recommendations == \"Y\": \n",
    "        print(\"\\nOk. Here are some more product recommendations.\") \n",
    "        if (len(greatest_similarity_product_ids_list) == 3) | (len(greatest_similarity_product_ids_list) > 3): \n",
    "\n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "            print(\"Product: \", products_df[\"product_id\"].iloc[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "            print(\"Product: \", products_df[\"product_id\"].iloc[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[3]] \n",
    "            print(\"Product: \", products_df[\"product_id\"].iloc[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        elif len(greatest_similarity_product_ids_list) == 2: \n",
    "\n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "            print(\"Product: \", products_df[\"product_id\"].iloc[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "            print(\"Product: \", products_df[\"product_id\"].iloc[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        else: \n",
    "            moree = True \n",
    "            \n",
    "            more_detail = input(\"\\nThere are no more similar products. What category is your product in? Enter number.\") \n",
    "    \n",
    "            # a dictionary with a number associated with each product category \n",
    "            product_categories_dict = {} \n",
    "        \n",
    "            # array with unique product categories \n",
    "            product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "        \n",
    "            for i in range(len(product_categories)): \n",
    "                product_categories_dict[i+1] = product_categories[i] \n",
    "        \n",
    "            # ask user for product category \n",
    "            print(\"\\nWhich category is the product in? Enter in number.\") \n",
    "            for keys, values in product_categories_dict.items(): \n",
    "                print(f\"{keys}: {values}\") \n",
    "            \n",
    "            user_input_2 = input(\"\") \n",
    "            \n",
    "            user_input_2 = int(user_input_2.strip()) \n",
    "            \n",
    "            user_input_product_cat = product_categories_dict.get(user_input_2) \n",
    "\n",
    "    else: \n",
    "        if (more_detail_user_input != \"Y\") & (another != \"Y\"): \n",
    "            print(\"\\nHave a great day! :)\") \n",
    "            break \n",
    "    \n",
    "    if moree == True: \n",
    "        user_product_category = df[df[\"product_cat_name_translated\"] == user_input_product_cat] \n",
    "        ranked_products = user_product_category.sort_values(by=\"review_score\", ascending=False) \n",
    "        \n",
    "        print(\"\\nHere are the three highest ranked products in that category.\") \n",
    "        print(ranked_products.head(3)[\"product_id\"]) \n",
    "\n",
    "        good2 = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "        good2 = good2.upper().strip() \n",
    "    \n",
    "        if good2 == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "                \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "                break \n",
    "            \n",
    "        else: \n",
    "            token_vectors = []; \n",
    "            print(\"\\nWe're sorry.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10c3c9-4215-4194-9331-896d38a044eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
