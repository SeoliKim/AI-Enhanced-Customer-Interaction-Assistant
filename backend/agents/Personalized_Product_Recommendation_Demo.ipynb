{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79a7e50-47a9-4dee-a3c6-80309ef1f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Found existing installation: numpy 1.26.0\n",
      "Uninstalling numpy-1.26.0:\n",
      "  Successfully uninstalled numpy-1.26.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/Users/jocelyn/miniconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (53 kB)\n",
      "Using cached numpy-1.26.0-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/Users/jocelyn/miniconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tableone 0.9.1 requires openpyxl>=3.1.2, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --yes\n",
      "Requirement already satisfied: gensim in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /Users/jocelyn/miniconda3/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "!pip install scikit-learn \n",
    "!pip uninstall numpy --yes \n",
    "!pip install numpy==1.26.0 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score \n",
    "!pip install gensim --yes \n",
    "!pip install gensim \n",
    "import gensim \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import time \n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f721128-d8b7-44f9-bd6b-595e1d559720",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/jocelyn/Desktop/DXC-4 Technology /AI-Enhanced-Customer-Interaction-Assistant/data/Final_db.csv\" \n",
    "df = pd.read_csv(filename, header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b50269f-46a8-4f21-8168-995451dc647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-0.10439922,  0.13158454, -0.2001088 ,  0.01089589, -0.06955387,\n",
      "       -0.4117542 ,  0.01581176,  0.91423404, -0.71401894, -0.85532063,\n",
      "       -0.09743784, -0.10605977,  0.20183387,  0.02999123, -0.00934011,\n",
      "       -0.08430766,  0.38189006, -0.22335796, -0.24631536, -0.61558723,\n",
      "        0.1583482 , -0.5293387 ,  0.608945  , -0.03829361, -0.20074442,\n",
      "        0.37492   , -0.49715137, -0.33835596, -0.263556  ,  0.19799344,\n",
      "        0.27581564,  0.14259148, -0.05737156, -0.35996908, -0.20203114,\n",
      "        0.18417025, -0.20313096, -0.17077069, -0.00410416, -0.9368315 ,\n",
      "       -0.11177359, -0.05215451,  0.6332973 , -0.09807709,  0.4742301 ,\n",
      "       -0.18044378,  0.21082816, -0.06798203,  0.78128874,  0.3661257 ,\n",
      "       -0.11489596,  0.37198633, -0.64725053, -0.41031462, -0.34295475,\n",
      "        0.21064213,  0.15305308,  0.02498494, -0.6022424 , -0.06397452,\n",
      "        0.17033975,  0.3641656 ,  0.19882824,  0.6801441 , -0.09343807,\n",
      "        0.735611  ,  0.37546045,  0.7237447 , -0.81276333,  0.25907066,\n",
      "        0.06227555,  0.62885094,  0.57595557,  0.13330233,  0.8254713 ,\n",
      "       -0.37608802,  0.04770716,  0.3181841 ,  0.16978893,  0.08424552,\n",
      "       -0.15477116, -0.17857185, -0.57779896,  0.6052368 , -0.42508578,\n",
      "       -0.64825547,  0.42397514,  0.89269364,  0.6256706 , -0.00554847,\n",
      "        0.42422158,  0.6876856 ,  0.01980618, -0.18970214,  0.48054162,\n",
      "       -0.04275664, -0.06918535, -0.5046107 ,  0.24066618, -0.11704946],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-4.5805699e-01,  2.3451789e-01, -1.7677636e-01, -3.7553313e-01,\n",
      "        2.4098510e-01, -7.0944381e-01, -5.5312026e-02,  9.0702885e-01,\n",
      "       -4.0204978e-01,  5.1417863e-01, -1.2753829e-01, -8.1145185e-01,\n",
      "        3.5021257e-01,  4.0597385e-01,  3.4485480e-01, -7.4535334e-01,\n",
      "       -6.8035088e-03, -2.7095139e-01,  4.7535157e-01, -6.0850245e-01,\n",
      "       -1.7253457e-01,  2.2787075e-02,  1.6499199e-02, -2.3834954e-01,\n",
      "       -4.7644258e-01,  9.7824401e-01, -1.0762414e-01, -8.3702636e-01,\n",
      "       -2.7440473e-01,  8.7390468e-04,  3.6496580e-01,  1.5698813e-02,\n",
      "        5.7243109e-01,  6.7525819e-02, -1.8461376e-01,  1.8033315e-01,\n",
      "        2.8593078e-01, -2.9157192e-01, -1.9342124e-01, -6.9948429e-01,\n",
      "       -8.1497833e-02, -6.6518790e-01, -2.8780502e-01,  3.0535138e-01,\n",
      "       -2.1173733e-01, -3.7834340e-01, -2.9339361e-01, -8.4620401e-02,\n",
      "       -6.8364047e-02, -3.9617279e-01,  1.6682917e-01, -2.1536292e-01,\n",
      "        2.7808252e-01,  2.2577813e-01,  4.4483028e-02,  1.5558232e-01,\n",
      "       -3.1495804e-01, -8.6629510e-02, -8.1318688e-01,  3.1499130e-01,\n",
      "        5.1875055e-01, -2.9314131e-01,  5.1600929e-02,  5.9678572e-01,\n",
      "       -7.3412144e-01,  5.1627445e-01, -2.6755220e-01, -8.3496332e-02,\n",
      "       -1.1703802e+00,  7.8567469e-01, -9.4745648e-01,  5.0563431e-01,\n",
      "        6.8029690e-01, -6.6927779e-01, -6.1319541e-02,  3.9048707e-01,\n",
      "       -3.8260633e-01,  3.0208302e-01, -6.1456239e-01, -7.1092434e-02,\n",
      "        7.7050917e-02,  2.1544483e-01, -9.5052522e-01,  2.5794649e-01,\n",
      "       -3.7680912e-01,  7.6686427e-02,  6.4393975e-02,  1.0134146e+00,\n",
      "       -7.7568367e-04, -2.1683444e-01,  9.1177201e-01,  1.2227903e-01,\n",
      "        4.0692824e-01,  5.1929313e-01,  7.3334980e-01,  4.9576208e-02,\n",
      "       -2.0314761e-01,  1.0200057e-01,  3.2272130e-01, -6.8784177e-02],\n",
      "      dtype=float32), array([-0.60211545,  0.36338842,  0.02981482,  0.01691464,  0.37991333,\n",
      "       -0.99894166,  0.3493693 ,  0.6866348 , -0.63223475,  0.04424162,\n",
      "        0.12007567, -0.11939678,  0.4760042 ,  0.00349821,  0.66440696,\n",
      "       -0.38032642,  0.77045655, -0.39339972, -0.33990863, -1.886846  ,\n",
      "        0.60981065, -0.07249913,  0.98171115, -0.071339  , -0.513797  ,\n",
      "        0.50822645, -0.24783574, -0.36310872, -0.21651244,  0.41599822,\n",
      "        0.800303  ,  0.5518035 ,  0.21262957, -0.20765953, -0.24367543,\n",
      "        0.58189625, -0.32209906, -0.3221489 , -0.5855873 , -0.40857327,\n",
      "        0.41113713, -0.43562388,  0.9865833 ,  0.22200698,  0.4956448 ,\n",
      "        0.04437682, -0.13223131, -0.3327311 ,  0.5128328 , -0.13806508,\n",
      "        0.9159346 , -0.5430307 , -0.40565935, -0.09681334, -0.8871422 ,\n",
      "       -0.12440056, -0.30639252,  0.2965503 , -0.7073417 ,  0.19918056,\n",
      "        0.13157932,  0.15226793,  0.5261431 ,  0.11428732, -0.18074636,\n",
      "        0.44721302,  0.13534717,  0.7918234 , -1.0571088 ,  0.20683174,\n",
      "       -0.40145788,  0.5937112 , -0.05636188,  0.46236324,  0.9089306 ,\n",
      "        0.03999763,  0.08722132,  0.9018088 , -0.4511017 ,  0.26446328,\n",
      "       -0.07284258,  0.41502854, -0.5621667 ,  0.3940874 ,  0.13073786,\n",
      "       -0.23289926,  0.32000735,  0.8184871 ,  0.16457078, -0.27526292,\n",
      "        1.3168116 ,  0.27655324,  0.05599659,  0.24393015,  0.5481749 ,\n",
      "       -0.10759819,  0.73859763, -0.3855852 , -0.6727442 ,  0.246794  ],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-0.6710365 ,  0.39473864,  0.10764923,  0.7622215 , -0.36450696,\n",
      "       -0.43101883, -0.26706818,  1.2927804 , -0.63946354, -0.83211154,\n",
      "       -0.19551675, -0.77548283,  0.11459646,  0.4603661 ,  0.22339518,\n",
      "       -0.3357832 ,  0.53246814, -0.30303425,  0.4063929 , -0.7238479 ,\n",
      "       -0.6257412 , -0.27189773, -0.19167529,  0.03771387, -0.41861463,\n",
      "       -0.8706687 , -0.9472323 , -0.25752804, -0.5975751 ,  0.41069403,\n",
      "        0.832469  , -0.04729897, -0.86027867, -0.33221778, -0.3743619 ,\n",
      "        0.4620975 , -0.00807939, -0.20015056,  0.1552842 , -0.73205227,\n",
      "        0.27753592, -0.15067916, -0.4360762 , -0.29249728,  0.790823  ,\n",
      "        0.17194383,  0.36251464,  0.193403  ,  1.0816779 ,  0.3451105 ,\n",
      "        0.26123   , -0.4279838 , -0.36472145,  0.4165943 ,  0.19148874,\n",
      "        0.9757224 ,  0.59585255,  0.09950177, -0.47763076,  0.4346758 ,\n",
      "       -0.54761267,  0.9400437 , -0.6319998 , -0.28387216,  0.28524598,\n",
      "       -0.6877931 , -0.03547213, -0.15010186, -0.90080094, -0.06945867,\n",
      "        0.4802121 ,  0.59592164,  1.1186038 ,  0.2294566 ,  0.82761765,\n",
      "        0.0758346 , -0.11634301, -0.48378825, -0.41753912, -0.16839993,\n",
      "       -0.22371204, -0.737819  , -0.00641705,  0.9041333 , -0.2547571 ,\n",
      "       -0.31066537, -0.4284539 ,  1.2505063 ,  1.189737  ,  1.0692831 ,\n",
      "        0.90732265,  0.90374684, -0.20798725,  0.8961742 ,  0.6713663 ,\n",
      "        0.8620236 ,  0.02794502,  0.0755018 ,  0.05571808, -0.3211749 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# check to see if there are products with multiple orders \n",
    "product_id_value_counts = df[\"product_id\"].value_counts() \n",
    "\n",
    "# store the products with multiple orders in products_multiple_orders \n",
    "products_multiple_orders = product_id_value_counts[product_id_value_counts > 1] \n",
    "\n",
    "# create a new column in df that stores the number of times each item was purchased \n",
    "df[\"times_purchased\"] = df[\"product_id\"].map(product_id_value_counts) \n",
    "\n",
    "# stores all of the unique product categories into the array \"product_categories\" \n",
    "product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "\n",
    "# fill in rows in df[\"review_comment_title\"] column that have na values with \"\" \n",
    "df[\"review_comment_title\"] = df[\"review_comment_title\"].fillna(\"\") \n",
    "\n",
    "# fill in rows in df[\"review_comment_message\"] column that have na values with \"\" \n",
    "df[\"review_comment_message\"] = df[\"review_comment_message\"].fillna(\"\") \n",
    "\n",
    "# merge the product reviews and product review titles with a space in between both \n",
    "df[\"merged_review_messages_and_titles\"] = df[\"review_comment_message\"] + \" \" + df[\"review_comment_title\"] \n",
    "df[\"merged_review_messages_and_titles\"] = df[\"merged_review_messages_and_titles\"].str.strip() \n",
    "\n",
    "# dictionary to store dataframes of the product reviews for all products in each category \n",
    "# products are separated by category \n",
    "product_reviews_by_category = {}; \n",
    "\n",
    "# fill in dictionary with keys (df category) and values (product_id and product reviews for all products in that category) \n",
    "for category in product_categories: \n",
    "    product_reviews_by_category[\"df_\" + str(category)] = df[df[\"product_cat_name_translated\"] == category][[\"product_id\", \"merged_review_messages_and_titles\"]] \n",
    "\n",
    "# feature ~ product title and review \n",
    "X = df[\"merged_review_messages_and_titles\"] \n",
    "\n",
    "# label ~ product category \n",
    "y = df[\"product_cat_name_translated\"] \n",
    "\n",
    "# preprocess each (each row) product title and review \n",
    "X = X.apply(lambda row: gensim.utils.simple_preprocess(row)) \n",
    "\n",
    "# split data into a training set and a test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1111) \n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(X_train, vector_size=100, window=5, min_count=1) \n",
    "\n",
    "# replace every word in the dataset that's also found in word2vec_model with their word embedding \n",
    "# a set with all of the unique words in the word2vex_model \n",
    "words = set(word2vec_model.wv.index_to_key) \n",
    "\n",
    "# X_train word embeddings \n",
    "# array for all of the word embeddings for X_train \n",
    "X_train_word_embeddings = []; \n",
    "for row in X_train: \n",
    "    row_embedding = [] \n",
    "    for word in words: \n",
    "        if word in row: \n",
    "            row_embedding.append(word2vec_model.wv[word]) \n",
    "    X_train_word_embeddings.append(np.array(row_embedding)) \n",
    "\n",
    "# X_test word embeddings \n",
    "# array for all of the word embeddings for X_test \n",
    "X_test_word_embeddings = []; \n",
    "for row in X_test: \n",
    "    row_embedding = []; \n",
    "    for word in words: \n",
    "        if word in row: \n",
    "            row_embedding.append(word2vec_model.wv[word]) \n",
    "    X_test_word_embeddings.append(np.array(row_embedding)) \n",
    "\n",
    "X_train_feature_vector = []\n",
    "for w in X_train_word_embeddings:\n",
    "    if w.size:\n",
    "        X_train_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_train_feature_vector.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_feature_vector = []\n",
    "for w in X_test_word_embeddings:\n",
    "    if w.size:\n",
    "        X_test_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_test_feature_vector.append(np.zeros(100, dtype=float))\n",
    "\n",
    "print(X_train_feature_vector[1:11]) \n",
    "\n",
    "# logistic regression \n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_feature_vector, y_train)\n",
    "\n",
    "probability_predictions = model.predict_proba(X_test_feature_vector) \n",
    "\n",
    "# predict most likely product category \n",
    "class_label_predictions = model.predict(X_test_feature_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996b3752-7df3-42a4-8002-5e2a20da1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to the personalized product recommendation chatbot.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What product are you looking for? book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response ...\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "        print(type(word2vec_prediction)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d6d0b3-1f51-4691-9b13-32ca5ffb6e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to the personalized product recommendation chatbot.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What product are you looking for? health\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response ...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "We can't find a related product. Please input more detail. book\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     32\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWe can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a related product. Please input more detail.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m---> 34\u001b[0m word2vec_prediction_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mword2vec_prediction\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \u001b[39;00m\n\u001b[1;32m     38\u001b[0m product_reviews_by_category[word2vec_prediction_str] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "        print(word2vec_prediction) \n",
    "    \n",
    "    else: \n",
    "        user_input = input(\"\\nWe can't find a related product. Please input more detail.\") \n",
    "\n",
    "    word2vec_prediction_str = \"df_\" + str(word2vec_prediction[0]) \n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    product_reviews_by_category[word2vec_prediction_str] \n",
    "    \n",
    "    # use bigrams (for cases of negation like \"not good\") and trigrams (for cases with numbers like \"bought 3 pairs\") \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    # fit vectorizer to the product review titles and messages \n",
    "    vectorizer.fit(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "    \n",
    "    # result_matrix contains each document's TF_IDF scores \n",
    "    resulting_matrix = vectorizer.transform(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "        \n",
    "    product_id = product_reviews_by_category[word2vec_prediction_str][\"product_id\"].tolist() \n",
    "        \n",
    "    resulting_matrix = pd.DataFrame(resulting_matrix.toarray(), index=product_id, columns=vectorizer.get_feature_names_out()) \n",
    "    \n",
    "    # determine product in category that's most similar to user input \n",
    "    user_input_TF_IDF_vector = vectorizer.transform([user_input]) \n",
    "    # similarity of user input word vector and each of the rows' word vectors \n",
    "    similarity = cosine_similarity(user_input_TF_IDF_vector, resulting_matrix) \n",
    "    greatest_similarity_indices = np.argsort(similarity[0][::-1]) \n",
    "    greatest_similarity_product_ids = resulting_matrix.index[greatest_similarity_indices] \n",
    "    greatest_similarity_product_ids_list = greatest_similarity_product_ids.unique().tolist() \n",
    "    \n",
    "    # rows with that product id \n",
    "    products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[0]] \n",
    "    \n",
    "    clear_output(wait=True) \n",
    "    \n",
    "    print(\"\\nHow about this product?\") \n",
    "    print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "    print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "    print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "    more_detail_user_input = input(\"\\nDo you want more detail about this product? (Y/N)\") \n",
    "    \n",
    "    more_detail_user_input = more_detail_user_input.upper().strip() \n",
    "    \n",
    "    more_recommendations = \"N\" \n",
    "    more_detail = \"\" \n",
    "    moree = False \n",
    "    user_input_product_cat = \"\" \n",
    "    user_input = \"\" \n",
    "    \n",
    "    if more_detail_user_input == \"Y\": \n",
    "        print(\"\\nOk. Here is more detail about the product.\") \n",
    "        print(\"Total Orders: \", len(products_df)) \n",
    "        print(\"Product Weight (g): \", products_df[\"product_weight_g\"].iloc[0]) \n",
    "        print(\"Product Length (cm): \", products_df[\"product_length_cm\"].iloc[0]) \n",
    "        print(\"Product Height (cm): \", products_df[\"product_height_cm\"].iloc[0]) \n",
    "        print(\"Product Width (cm): \", products_df[\"product_width_cm\"].iloc[0]) \n",
    "        \n",
    "        good = input(\"\\nIs this a good recommendation? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "            \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "                break \n",
    "        \n",
    "        else: \n",
    "            print(\"\\nWe're sorry.\") \n",
    "            token_vectors = []; \n",
    "    \n",
    "    else: \n",
    "        more_recommendations = input(\"\\nWould you like more similar product recommendations? (Y/N)\") \n",
    "        more_recommendations = more_recommendations.upper().strip() \n",
    "    \n",
    "    if more_recommendations == \"Y\": \n",
    "        print(\"\\nOk. Here are some more product recommendations.\") \n",
    "        if (len(greatest_similarity_product_ids_list) == 3) | (len(greatest_similarity_product_ids_list) > 3): \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        elif len(greatest_similarity_product_ids_list) == 2: \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        else: \n",
    "            moree = True \n",
    "            \n",
    "            more_detail = input(\"\\nThere are no more similar products. What category is your product in? Enter number.\") \n",
    "    \n",
    "            # a dictionary with a number associated with each product category \n",
    "            product_categories_dict = {} \n",
    "        \n",
    "            # array with unique product categories \n",
    "            product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "        \n",
    "            for i in range(len(product_categories)): \n",
    "                product_categories_dict[i+1] = product_categories[i] \n",
    "        \n",
    "            # ask user for product category \n",
    "            print(\"\\nWhich category is the product in? Enter in number.\") \n",
    "            for keys, values in product_categories_dict.items(): \n",
    "                print(f\"{keys}: {values}\") \n",
    "            \n",
    "            user_input_2 = input(\"\") \n",
    "            \n",
    "            user_input_2 = int(user_input_2.strip()) \n",
    "            \n",
    "            user_input_product_cat = product_categories_dict.get(user_input_2) \n",
    "\n",
    "    else: \n",
    "        if (more_detail_user_input != \"Y\") & (another != \"Y\"): \n",
    "            print(\"\\nHave a great day! :)\") \n",
    "            break \n",
    "    \n",
    "    if moree == True: \n",
    "        user_product_category = df[df[\"product_cat_name_translated\"] == user_input_product_cat] \n",
    "        ranked_products = user_product_category.sort_values(by=\"review_score\", ascending=False) \n",
    "        \n",
    "        print(\"\\nHere are the three highest ranked products in that category.\") \n",
    "        print(ranked_products.head(3)[\"product_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f9c3d0-5790-41c7-aced-2e18adff1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How about this product?\n",
      "Product:  10adb53d8faa890ca7c2f0cbcb68d777\n",
      "Price:  19.9\n",
      "Average Product Rating:  3.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrice: \u001b[39m\u001b[38;5;124m\"\u001b[39m, products_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Product Rating: \u001b[39m\u001b[38;5;124m\"\u001b[39m, products_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \n\u001b[0;32m---> 88\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDo you want more detail about this product? (Y/N)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     90\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m more_detail_user_input\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mstrip() \n\u001b[1;32m     92\u001b[0m more_recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "    \n",
    "    else: \n",
    "        while len(token_vectors) == 0: \n",
    "            user_input = input(\"\\nWe can't find a related product. Please input more detail.\") \n",
    "            user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "            token_vectors = [] \n",
    "    \n",
    "            # loop through all tokens from the tokenized version of the user input \n",
    "            for token in user_input_tokens: \n",
    "                # if the token is also found in the dataset \n",
    "                if token in word2vec_model.wv.key_to_index: \n",
    "                    # add its word vector to the \"token_vectors\" list \n",
    "                    token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "            # if there are vectors \n",
    "            if len(token_vectors) > 0: \n",
    "                # calculate the mean for each token_vector and combine into an array \n",
    "                overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "                # reshape into a two dimensional array \n",
    "                overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "                word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "word2vec_prediction_str = \"df_\" + str(word2vec_prediction[0]) \n",
    "    \n",
    "#vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "product_reviews_by_category[word2vec_prediction_str] \n",
    "    \n",
    "# use bigrams (for cases of negation like \"not good\") and trigrams (for cases with numbers like \"bought 3 pairs\") \n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "# fit vectorizer to the product review titles and messages \n",
    "vectorizer.fit(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "    \n",
    "# result_matrix contains each document's TF_IDF scores \n",
    "resulting_matrix = vectorizer.transform(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "        \n",
    "product_id = product_reviews_by_category[word2vec_prediction_str][\"product_id\"].tolist() \n",
    "        \n",
    "resulting_matrix = pd.DataFrame(resulting_matrix.toarray(), index=product_id, columns=vectorizer.get_feature_names_out()) \n",
    "    \n",
    "# determine product in category that's most similar to user input \n",
    "user_input_TF_IDF_vector = vectorizer.transform([user_input]) \n",
    "# similarity of user input word vector and each of the rows' word vectors \n",
    "similarity = cosine_similarity(user_input_TF_IDF_vector, resulting_matrix) \n",
    "greatest_similarity_indices = np.argsort(similarity[0][::-1]) \n",
    "greatest_similarity_product_ids = resulting_matrix.index[greatest_similarity_indices] \n",
    "greatest_similarity_product_ids_list = greatest_similarity_product_ids.unique().tolist() \n",
    "    \n",
    "# rows with that product id \n",
    "products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[0]] \n",
    "    \n",
    "clear_output(wait=True) \n",
    "    \n",
    "print(\"\\nHow about this product?\") \n",
    "print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "more_detail_user_input = input(\"\\nDo you want more detail about this product? (Y/N)\") \n",
    "    \n",
    "more_detail_user_input = more_detail_user_input.upper().strip() \n",
    "    \n",
    "more_recommendations = \"N\" \n",
    "more_detail = \"\" \n",
    "moree = False \n",
    "user_input_product_cat = \"\" \n",
    "user_input = \"\" \n",
    "    \n",
    "if more_detail_user_input == \"Y\": \n",
    "    print(\"\\nOk. Here is more detail about the product.\") \n",
    "    print(\"Total Orders: \", len(products_df)) \n",
    "    print(\"Product Weight (g): \", products_df[\"product_weight_g\"].iloc[0]) \n",
    "    print(\"Product Length (cm): \", products_df[\"product_length_cm\"].iloc[0]) \n",
    "    print(\"Product Height (cm): \", products_df[\"product_height_cm\"].iloc[0]) \n",
    "    print(\"Product Width (cm): \", products_df[\"product_width_cm\"].iloc[0]) \n",
    "        \n",
    "    good = input(\"\\nIs this a good recommendation? (Y/N)\") \n",
    "    good = good.upper().strip() \n",
    "    \n",
    "    if good == \"Y\": \n",
    "        another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "        another = another.upper().strip() \n",
    "            \n",
    "        if another == \"Y\": \n",
    "            token_vectors = []; \n",
    "        else: \n",
    "            print(\"\\nHave a great day! :)\") \n",
    "        \n",
    "    else: \n",
    "        print(\"\\nWe're sorry.\") \n",
    "        token_vectors = []; \n",
    "\n",
    "else: \n",
    "    more_recommendations = input(\"\\nWould you like more similar product recommendations? (Y/N)\") \n",
    "    more_recommendations = more_recommendations.upper().strip() \n",
    "    \n",
    "if more_recommendations == \"Y\": \n",
    "    print(\"\\nOk. Here are some more product recommendations.\") \n",
    "    if (len(greatest_similarity_product_ids_list) == 3) | (len(greatest_similarity_product_ids_list) > 3): \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "        good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "                \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "            \n",
    "        else: \n",
    "            token_vectors = []; \n",
    "            print(\"\\nWe're sorry.\") \n",
    "    \n",
    "    elif len(greatest_similarity_product_ids_list) == 2: \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "        good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "                \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "            \n",
    "        else: \n",
    "            token_vectors = []; \n",
    "            print(\"\\nWe're sorry.\") \n",
    "    \n",
    "    else: \n",
    "        moree = True \n",
    "            \n",
    "        more_detail = input(\"\\nThere are no more similar products. What category is your product in? Enter number.\") \n",
    "    \n",
    "        # a dictionary with a number associated with each product category \n",
    "        product_categories_dict = {} \n",
    "        \n",
    "        # array with unique product categories \n",
    "        product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "        \n",
    "        for i in range(len(product_categories)): \n",
    "            product_categories_dict[i+1] = product_categories[i] \n",
    "        \n",
    "        # ask user for product category \n",
    "        print(\"\\nWhich category is the product in? Enter in number.\") \n",
    "        for keys, values in product_categories_dict.items(): \n",
    "            print(f\"{keys}: {values}\") \n",
    "            \n",
    "        user_input_2 = input(\"\") \n",
    "            \n",
    "        user_input_2 = int(user_input_2.strip()) \n",
    "            \n",
    "        user_input_product_cat = product_categories_dict.get(user_input_2) \n",
    "\n",
    "else: \n",
    "    if (more_detail_user_input != \"Y\") & (another != \"Y\"): \n",
    "        print(\"\\nHave a great day! :)\") \n",
    "    \n",
    "if moree == True: \n",
    "    user_product_category = df[df[\"product_cat_name_translated\"] == user_input_product_cat] \n",
    "    ranked_products = user_product_category.sort_values(by=\"review_score\", ascending=False) \n",
    "        \n",
    "    print(\"\\nHere are the three highest ranked products in that category.\") \n",
    "    print(ranked_products.head(3)[\"product_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dbc059-6a8c-4952-957d-d0f5ddc97e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How about this product?\n",
      "Product:  10adb53d8faa890ca7c2f0cbcb68d777\n",
      "Price:  19.9\n",
      "Average Product Rating:  3.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrice: \u001b[39m\u001b[38;5;124m\"\u001b[39m, products_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Product Rating: \u001b[39m\u001b[38;5;124m\"\u001b[39m, products_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \n\u001b[0;32m---> 88\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDo you want more detail about this product? (Y/N)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     90\u001b[0m more_detail_user_input \u001b[38;5;241m=\u001b[39m more_detail_user_input\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mstrip() \n\u001b[1;32m     92\u001b[0m more_recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "    \n",
    "    else: \n",
    "        while len(token_vectors) == 0: \n",
    "            user_input = input(\"\\nWe can't find a related product. Please input more detail.\") \n",
    "            user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "            token_vectors = [] \n",
    "    \n",
    "            # loop through all tokens from the tokenized version of the user input \n",
    "            for token in user_input_tokens: \n",
    "                # if the token is also found in the dataset \n",
    "                if token in word2vec_model.wv.key_to_index: \n",
    "                    # add its word vector to the \"token_vectors\" list \n",
    "                    token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "            # if there are vectors \n",
    "            if len(token_vectors) > 0: \n",
    "                # calculate the mean for each token_vector and combine into an array \n",
    "                overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "                # reshape into a two dimensional array \n",
    "                overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "                word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "word2vec_prediction_str = \"df_\" + str(word2vec_prediction[0]) \n",
    "    \n",
    "#vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "product_reviews_by_category[word2vec_prediction_str] \n",
    "    \n",
    "# use bigrams (for cases of negation like \"not good\") and trigrams (for cases with numbers like \"bought 3 pairs\") \n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "# fit vectorizer to the product review titles and messages \n",
    "vectorizer.fit(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "    \n",
    "# result_matrix contains each document's TF_IDF scores \n",
    "resulting_matrix = vectorizer.transform(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "        \n",
    "product_id = product_reviews_by_category[word2vec_prediction_str][\"product_id\"].tolist() \n",
    "        \n",
    "resulting_matrix = pd.DataFrame(resulting_matrix.toarray(), index=product_id, columns=vectorizer.get_feature_names_out()) \n",
    "    \n",
    "# determine product in category that's most similar to user input \n",
    "user_input_TF_IDF_vector = vectorizer.transform([user_input]) \n",
    "# similarity of user input word vector and each of the rows' word vectors \n",
    "similarity = cosine_similarity(user_input_TF_IDF_vector, resulting_matrix) \n",
    "greatest_similarity_indices = np.argsort(similarity[0][::-1]) \n",
    "greatest_similarity_product_ids = resulting_matrix.index[greatest_similarity_indices] \n",
    "greatest_similarity_product_ids_list = greatest_similarity_product_ids.unique().tolist() \n",
    "    \n",
    "# rows with that product id \n",
    "products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[0]] \n",
    "    \n",
    "clear_output(wait=True) \n",
    "    \n",
    "print(\"\\nHow about this product?\") \n",
    "print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "more_detail_user_input = input(\"\\nDo you want more detail about this product? (Y/N)\") \n",
    "    \n",
    "more_detail_user_input = more_detail_user_input.upper().strip() \n",
    "    \n",
    "more_recommendations = \"N\" \n",
    "more_detail = \"\" \n",
    "moree = False \n",
    "user_input_product_cat = \"\" \n",
    "user_input = \"\" \n",
    "    \n",
    "if more_detail_user_input == \"Y\": \n",
    "    print(\"\\nOk. Here is more detail about the product.\") \n",
    "    print(\"Total Orders: \", len(products_df)) \n",
    "    print(\"Product Weight (g): \", products_df[\"product_weight_g\"].iloc[0]) \n",
    "    print(\"Product Length (cm): \", products_df[\"product_length_cm\"].iloc[0]) \n",
    "    print(\"Product Height (cm): \", products_df[\"product_height_cm\"].iloc[0]) \n",
    "    print(\"Product Width (cm): \", products_df[\"product_width_cm\"].iloc[0]) \n",
    "        \n",
    "    good = input(\"\\nIs this a good recommendation? (Y/N)\") \n",
    "    good = good.upper().strip() \n",
    "    \n",
    "    if good == \"Y\": \n",
    "        another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "        another = another.upper().strip() \n",
    "            \n",
    "        if another == \"Y\": \n",
    "            token_vectors = []; \n",
    "        else: \n",
    "            print(\"\\nHave a great day! :)\") \n",
    "        \n",
    "    else: \n",
    "        print(\"\\nWe're sorry.\") \n",
    "        token_vectors = []; \n",
    "\n",
    "else: \n",
    "    more_recommendations = input(\"\\nWould you like more similar product recommendations? (Y/N)\") \n",
    "    more_recommendations = more_recommendations.upper().strip() \n",
    "    \n",
    "if more_recommendations == \"Y\": \n",
    "    print(\"\\nOk. Here are some more product recommendations.\") \n",
    "    if (len(greatest_similarity_product_ids_list) == 3) | (len(greatest_similarity_product_ids_list) > 3): \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "        good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "                \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "            \n",
    "        else: \n",
    "            token_vectors = []; \n",
    "            print(\"\\nWe're sorry.\") \n",
    "    \n",
    "    elif len(greatest_similarity_product_ids_list) == 2: \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "        products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "        print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "        print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "        good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "                \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "            \n",
    "        else: \n",
    "            token_vectors = []; \n",
    "            print(\"\\nWe're sorry.\") \n",
    "    \n",
    "    else: \n",
    "        moree = True \n",
    "            \n",
    "        more_detail = input(\"\\nThere are no more similar products. What category is your product in? Enter number.\") \n",
    "    \n",
    "        # a dictionary with a number associated with each product category \n",
    "        product_categories_dict = {} \n",
    "        \n",
    "        # array with unique product categories \n",
    "        product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "        \n",
    "        for i in range(len(product_categories)): \n",
    "            product_categories_dict[i+1] = product_categories[i] \n",
    "        \n",
    "        # ask user for product category \n",
    "        print(\"\\nWhich category is the product in? Enter in number.\") \n",
    "        for keys, values in product_categories_dict.items(): \n",
    "            print(f\"{keys}: {values}\") \n",
    "            \n",
    "        user_input_2 = input(\"\") \n",
    "            \n",
    "        user_input_2 = int(user_input_2.strip()) \n",
    "            \n",
    "        user_input_product_cat = product_categories_dict.get(user_input_2) \n",
    "\n",
    "else: \n",
    "    if (more_detail_user_input != \"Y\") & (another != \"Y\"): \n",
    "        print(\"\\nHave a great day! :)\") \n",
    "    \n",
    "if moree == True: \n",
    "    user_product_category = df[df[\"product_cat_name_translated\"] == user_input_product_cat] \n",
    "    ranked_products = user_product_category.sort_values(by=\"review_score\", ascending=False) \n",
    "        \n",
    "    print(\"\\nHere are the three highest ranked products in that category.\") \n",
    "    print(ranked_products.head(3)[\"product_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05707038-5165-4e3d-bd68-5f006297e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How about this product?\n",
      "Product:  10adb53d8faa890ca7c2f0cbcb68d777\n",
      "Price:  19.9\n",
      "Average Product Rating:  3.85\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello! Welcome to the personalized product recommendation chatbot.\") \n",
    "\n",
    "# create a list containing the vectors for the tokens/words found in the user input that are also found in the dataset of available products \n",
    "token_vectors = [] \n",
    "\n",
    "while len(token_vectors) == 0: \n",
    "    another = \"\"\n",
    "    user_input = input(\"\\nWhat product are you looking for?\") \n",
    "\n",
    "    print(\"Generating response ...\") \n",
    "\n",
    "    user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "    token_vectors = [] \n",
    "    \n",
    "    # loop through all tokens from the tokenized version of the user input \n",
    "    for token in user_input_tokens: \n",
    "        # if the token is also found in the dataset \n",
    "        if token in word2vec_model.wv.key_to_index: \n",
    "            # add its word vector to the \"token_vectors\" list \n",
    "            token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "    # if there are vectors \n",
    "    if len(token_vectors) > 0: \n",
    "        # calculate the mean for each token_vector and combine into an array \n",
    "        overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "        # reshape into a two dimensional array \n",
    "        overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "        word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "    else: \n",
    "        while len(token_vectors) == 0: \n",
    "            user_input = input(\"\\nWe can't find a related product. Please input more detail.\") \n",
    "            user_input_tokens = list(gensim.utils.simple_preprocess(user_input)) \n",
    "            token_vectors = [] \n",
    "    \n",
    "            # loop through all tokens from the tokenized version of the user input \n",
    "            for token in user_input_tokens: \n",
    "                # if the token is also found in the dataset \n",
    "                if token in word2vec_model.wv.key_to_index: \n",
    "                    # add its word vector to the \"token_vectors\" list \n",
    "                    token_vectors.append(word2vec_model.wv[token]) \n",
    "    \n",
    "            # if there are vectors \n",
    "            if len(token_vectors) > 0: \n",
    "                # calculate the mean for each token_vector and combine into an array \n",
    "                overall_token_vector = np.mean(token_vectors, axis=0) \n",
    "                # reshape into a two dimensional array \n",
    "                overall_token_vector = overall_token_vector.reshape(1, -1) \n",
    "                word2vec_prediction = model.predict(overall_token_vector) \n",
    "\n",
    "    word2vec_prediction_str = \"df_\" + str(word2vec_prediction[0]) \n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    product_reviews_by_category[word2vec_prediction_str] \n",
    "    \n",
    "    # use bigrams (for cases of negation like \"not good\") and trigrams (for cases with numbers like \"bought 3 pairs\") \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "    \n",
    "    # fit vectorizer to the product review titles and messages \n",
    "    vectorizer.fit(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "    \n",
    "    # result_matrix contains each document's TF_IDF scores \n",
    "    resulting_matrix = vectorizer.transform(product_reviews_by_category[word2vec_prediction_str][\"merged_review_messages_and_titles\"]) \n",
    "        \n",
    "    product_id = product_reviews_by_category[word2vec_prediction_str][\"product_id\"].tolist() \n",
    "        \n",
    "    resulting_matrix = pd.DataFrame(resulting_matrix.toarray(), index=product_id, columns=vectorizer.get_feature_names_out()) \n",
    "    \n",
    "    # determine product in category that's most similar to user input \n",
    "    user_input_TF_IDF_vector = vectorizer.transform([user_input]) \n",
    "    # similarity of user input word vector and each of the rows' word vectors \n",
    "    similarity = cosine_similarity(user_input_TF_IDF_vector, resulting_matrix) \n",
    "    greatest_similarity_indices = np.argsort(similarity[0][::-1]) \n",
    "    greatest_similarity_product_ids = resulting_matrix.index[greatest_similarity_indices] \n",
    "    greatest_similarity_product_ids_list = greatest_similarity_product_ids.unique().tolist() \n",
    "    \n",
    "    # rows with that product id \n",
    "    products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[0]] \n",
    "    \n",
    "    clear_output(wait=True) \n",
    "    \n",
    "    print(\"\\nHow about this product?\") \n",
    "    print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "    print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "    print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "    more_detail_user_input = input(\"\\nDo you want more detail about this product? (Y/N)\") \n",
    "    \n",
    "    more_detail_user_input = more_detail_user_input.upper().strip() \n",
    "    \n",
    "    more_recommendations = \"N\" \n",
    "    more_detail = \"\" \n",
    "    moree = False \n",
    "    user_input_product_cat = \"\" \n",
    "    user_input = \"\" \n",
    "    \n",
    "    if more_detail_user_input == \"Y\": \n",
    "        print(\"\\nOk. Here is more detail about the product.\") \n",
    "        print(\"Total Orders: \", len(products_df)) \n",
    "        print(\"Product Weight (g): \", products_df[\"product_weight_g\"].iloc[0]) \n",
    "        print(\"Product Length (cm): \", products_df[\"product_length_cm\"].iloc[0]) \n",
    "        print(\"Product Height (cm): \", products_df[\"product_height_cm\"].iloc[0]) \n",
    "        print(\"Product Width (cm): \", products_df[\"product_width_cm\"].iloc[0]) \n",
    "        \n",
    "        good = input(\"\\nIs this a good recommendation? (Y/N)\") \n",
    "        good = good.upper().strip() \n",
    "    \n",
    "        if good == \"Y\": \n",
    "            another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "            another = another.upper().strip() \n",
    "            \n",
    "            if another == \"Y\": \n",
    "                token_vectors = []; \n",
    "            else: \n",
    "                print(\"\\nHave a great day! :)\") \n",
    "                break \n",
    "        \n",
    "        else: \n",
    "            print(\"\\nWe're sorry.\") \n",
    "            token_vectors = []; \n",
    "    \n",
    "    else: \n",
    "        more_recommendations = input(\"\\nWould you like more similar product recommendations? (Y/N)\") \n",
    "        more_recommendations = more_recommendations.upper().strip() \n",
    "    \n",
    "    if more_recommendations == \"Y\": \n",
    "        print(\"\\nOk. Here are some more product recommendations.\") \n",
    "        if (len(greatest_similarity_product_ids_list) == 3) | (len(greatest_similarity_product_ids_list) > 3): \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[3]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        elif len(greatest_similarity_product_ids_list) == 2: \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[1]] \n",
    "        \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean(), \"\\n\") \n",
    "    \n",
    "            products_df = df[df[\"product_id\"] == greatest_similarity_product_ids_list[2]] \n",
    "            print(\"Product: \", greatest_similarity_product_ids_list[0]) \n",
    "            print(\"Price: \", products_df[\"price\"].iloc[0]) \n",
    "            print(\"Average Product Rating: \", products_df[\"review_score\"].mean()) \n",
    "    \n",
    "            good = input(\"\\nWere any of these recommendations helpful? (Y/N)\") \n",
    "            good = good.upper().strip() \n",
    "    \n",
    "            if good == \"Y\": \n",
    "                another = input(\"\\nWe're glad! Would you like like help finding another product? (Y/N)\") \n",
    "                another = another.upper().strip() \n",
    "                \n",
    "                if another == \"Y\": \n",
    "                    token_vectors = []; \n",
    "                else: \n",
    "                    print(\"\\nHave a great day! :)\") \n",
    "                    break \n",
    "            \n",
    "            else: \n",
    "                token_vectors = []; \n",
    "                print(\"\\nWe're sorry.\") \n",
    "    \n",
    "        else: \n",
    "            moree = True \n",
    "            \n",
    "            more_detail = input(\"\\nThere are no more similar products. What category is your product in? Enter number.\") \n",
    "    \n",
    "            # a dictionary with a number associated with each product category \n",
    "            product_categories_dict = {} \n",
    "        \n",
    "            # array with unique product categories \n",
    "            product_categories = df[\"product_cat_name_translated\"].unique() \n",
    "        \n",
    "            for i in range(len(product_categories)): \n",
    "                product_categories_dict[i+1] = product_categories[i] \n",
    "        \n",
    "            # ask user for product category \n",
    "            print(\"\\nWhich category is the product in? Enter in number.\") \n",
    "            for keys, values in product_categories_dict.items(): \n",
    "                print(f\"{keys}: {values}\") \n",
    "            \n",
    "            user_input_2 = input(\"\") \n",
    "            \n",
    "            user_input_2 = int(user_input_2.strip()) \n",
    "            \n",
    "            user_input_product_cat = product_categories_dict.get(user_input_2) \n",
    "\n",
    "    else: \n",
    "        if (more_detail_user_input != \"Y\") & (another != \"Y\"): \n",
    "            print(\"\\nHave a great day! :)\") \n",
    "            break \n",
    "    \n",
    "    if moree == True: \n",
    "        user_product_category = df[df[\"product_cat_name_translated\"] == user_input_product_cat] \n",
    "        ranked_products = user_product_category.sort_values(by=\"review_score\", ascending=False) \n",
    "        \n",
    "        print(\"\\nHere are the three highest ranked products in that category.\") \n",
    "        print(ranked_products.head(3)[\"product_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc7b3b2f-9afa-4f91-88f5-730ce6a1d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17        3.0\n",
      "2912      3.0\n",
      "16107     1.0\n",
      "17982     3.0\n",
      "18168     5.0\n",
      "35359     3.0\n",
      "36151     3.0\n",
      "42178     2.0\n",
      "47354     2.0\n",
      "49223     1.0\n",
      "53242     1.0\n",
      "53563     1.0\n",
      "64059     1.0\n",
      "67787     6.0\n",
      "69422     1.0\n",
      "81452     1.0\n",
      "92507     3.0\n",
      "99816     1.0\n",
      "100412    1.0\n",
      "106459    3.0\n",
      "Name: payment_installments, dtype: float64\n",
      "order_id                             5ff96c15d0b717ac6ad1f3d77225a350\n",
      "customer_id                          19402a48fe860416adf93348aba37740\n",
      "order_status                                                delivered\n",
      "order_purchase_timestamp                          2018-07-25 17:44:10\n",
      "order_approved_at                                 2018-07-25 17:55:14\n",
      "order_delivered_carrier_date                      2018-07-26 13:16:00\n",
      "order_delivered_customer_date                     2018-07-30 15:52:25\n",
      "order_estimated_delivery_date                     2018-08-08 00:00:00\n",
      "review_id                            346e42116863ea64d51cef464d8f0c9c\n",
      "review_score                                                      5.0\n",
      "review_comment_title                                                 \n",
      "review_comment_message                                               \n",
      "review_creation_date                              2018-07-31 00:00:00\n",
      "review_answer_timestamp                           2018-08-02 21:15:12\n",
      "payment_sequential                                                1.0\n",
      "payment_type                                              credit_card\n",
      "payment_installments                                              3.0\n",
      "payment_value                                                    32.7\n",
      "order_item_id                                                     1.0\n",
      "product_id                           10adb53d8faa890ca7c2f0cbcb68d777\n",
      "seller_id                            1900267e848ceeba8fa32d80c1a5f5a8\n",
      "shipping_limit_date                               2018-07-27 17:55:14\n",
      "price                                                            19.9\n",
      "freight_value                                                    12.8\n",
      "product_category_name                                 cama_mesa_banho\n",
      "product_name_lenght                                              52.0\n",
      "product_description_lenght                                      155.0\n",
      "product_photos_qty                                                1.0\n",
      "product_weight_g                                                200.0\n",
      "product_length_cm                                                16.0\n",
      "product_height_cm                                                10.0\n",
      "product_width_cm                                                 16.0\n",
      "customer_unique_id                   e2dfa3127fedbbca9707b36304996dab\n",
      "customer_zip_code_prefix                                         4812\n",
      "customer_city                                               sao paulo\n",
      "customer_state                                                     SP\n",
      "seller_zip_code_prefix                                        14940.0\n",
      "seller_city                                                  ibitinga\n",
      "seller_state                                                       SP\n",
      "product_cat_name_translated                            bed_bath_table\n",
      "times_purchased                                                  20.0\n",
      "merged_review_messages_and_titles                                    \n",
      "Name: 17, dtype: object\n",
      "order_id                                        3dcab1487bcd1dbcc6c968fdfe0511fb\n",
      "customer_id                                     fbbd7759305c446d285fa335394e4809\n",
      "order_status                                                           delivered\n",
      "order_purchase_timestamp                                     2018-02-04 21:19:44\n",
      "order_approved_at                                            2018-02-04 21:35:23\n",
      "order_delivered_carrier_date                                 2018-02-05 21:33:42\n",
      "order_delivered_customer_date                                2018-02-06 15:42:58\n",
      "order_estimated_delivery_date                                2018-03-01 00:00:00\n",
      "review_id                                       147b25708597b86b84186668cf8b366a\n",
      "review_score                                                                 2.0\n",
      "review_comment_title                                                            \n",
      "review_comment_message               Chegou apenas um dos produtos que comprei. \n",
      "review_creation_date                                         2018-02-07 00:00:00\n",
      "review_answer_timestamp                                      2018-02-09 18:29:24\n",
      "payment_sequential                                                           1.0\n",
      "payment_type                                                         credit_card\n",
      "payment_installments                                                         1.0\n",
      "payment_value                                                              65.74\n",
      "order_item_id                                                                2.0\n",
      "product_id                                      10adb53d8faa890ca7c2f0cbcb68d777\n",
      "seller_id                                       1900267e848ceeba8fa32d80c1a5f5a8\n",
      "shipping_limit_date                                          2018-02-08 21:31:08\n",
      "price                                                                       19.9\n",
      "freight_value                                                               3.49\n",
      "product_category_name                                            cama_mesa_banho\n",
      "product_name_lenght                                                         52.0\n",
      "product_description_lenght                                                 155.0\n",
      "product_photos_qty                                                           1.0\n",
      "product_weight_g                                                           200.0\n",
      "product_length_cm                                                           16.0\n",
      "product_height_cm                                                           10.0\n",
      "product_width_cm                                                            16.0\n",
      "customer_unique_id                              1aa85e14d63e3f290326d01b5f1b106b\n",
      "customer_zip_code_prefix                                                    1452\n",
      "customer_city                                                          sao paulo\n",
      "customer_state                                                                SP\n",
      "seller_zip_code_prefix                                                   14940.0\n",
      "seller_city                                                             ibitinga\n",
      "seller_state                                                                  SP\n",
      "product_cat_name_translated                                       bed_bath_table\n",
      "times_purchased                                                             20.0\n",
      "merged_review_messages_and_titles     Chegou apenas um dos produtos que comprei.\n",
      "Name: 16107, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"product_id\"] == \"10adb53d8faa890ca7c2f0cbcb68d777\"][\"payment_installments\"]) \n",
    "print(df.iloc[17]) \n",
    "print(df.iloc[16107]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e9f4f-48b1-45fd-94e3-b0f33693e4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
